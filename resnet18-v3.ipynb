{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"},{"sourceId":7392733,"sourceType":"datasetVersion","datasetId":4297749},{"sourceId":7465251,"sourceType":"datasetVersion","datasetId":4317718},{"sourceId":163366179,"sourceType":"kernelVersion"}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#A51C30\"><b><span style='color:#FFFFFF'>1 |</span></b> <b>INTRODUCTION</b></div>","metadata":{}},{"cell_type":"code","source":"# %pip install iterative-stratification\n\nfrom tensorflow.keras import mixed_precision \nimport os\nimport gc\nimport ctypes\n\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense,GlobalAveragePooling1D)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import (ModelCheckpoint, TensorBoard, ReduceLROnPlateau,\n                                      CSVLogger, EarlyStopping)\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom collections import Counter\nfrom typing import Tuple, Optional, Callable\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.losses import KLDivergence\nfrom tensorflow.keras.utils import Sequence\nimport pandas as pd \n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:50:55.468281Z","iopub.execute_input":"2024-03-06T07:50:55.468618Z","iopub.status.idle":"2024-03-06T07:51:01.053563Z","shell.execute_reply.started":"2024-03-06T07:50:55.468586Z","shell.execute_reply":"2024-03-06T07:51:01.052722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **`P100 GPU`**: For High Performance Computing(HPC)\n- **`T4 GPU`**: For Deep Learning and AI tasks","metadata":{}},{"cell_type":"markdown","source":"### <b><span style='color:#A51C30'> 1.3 </span> New Chain</b> ","metadata":{}},{"cell_type":"markdown","source":"![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F16438831%2Fee75fb2b76b12fda5c559504eacc09b8%2FCentral.PNG?generation=1708318428173795&alt=media)","metadata":{}},{"cell_type":"markdown","source":"### <b><span style='color:#A51C30'> 1.4 </span> Updated!</b> \n\n<div style=\"border-radius:10px; border: #babab5 solid; padding: 15px; background-color:##A51C30; font-size:100%;\">\n    \nüìå **`VER1`**: Original Chain, 0~30Hz, [4Waveblcok + 1GRU] \n    \nüìå **`VER2`**: Adding Central[Cz,Fz,C4,C3], 0~30Hz, \n","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#A51C30\"><b><span style='color:#FFFFFF'>2 |</span></b> <b>GPUs Setting</b></div>","metadata":{}},{"cell_type":"code","source":"VER = 2\n\nimport os\nimport gc\nimport ctypes\n\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\nprint('tensorflow version:',tf.__version__)\n\n# CUDA 0,1\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n\n# gpu strategy\ngpus = tf.config.list_physical_devices('GPU')\nif len(gpus) <= 1:\n    strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\n    print(f'Using {len(gpus)} gpus')\nelse: \n    strategy = tf.distribute.MirroredStrategy()\n    print(f'Using {len(gpus)} gpus')\n    \n# warning filtering\nimport warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:51:01.054542Z","iopub.execute_input":"2024-03-06T07:51:01.054995Z","iopub.status.idle":"2024-03-06T07:51:01.928606Z","shell.execute_reply.started":"2024-03-06T07:51:01.054970Z","shell.execute_reply":"2024-03-06T07:51:01.927611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# mixed_preicision\n# Helps memeory effectively \n\nMIX=True\n\nif MIX: \n    tf.config.optimizer.set_experimental_options({'auto_mixed_precision':True})\n    print(\"Mixed Precision Enabled\")\nelse:\n    print(\"Using Full Precision\")","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:51:01.930915Z","iopub.execute_input":"2024-03-06T07:51:01.931399Z","iopub.status.idle":"2024-03-06T07:51:01.936703Z","shell.execute_reply.started":"2024-03-06T07:51:01.931364Z","shell.execute_reply":"2024-03-06T07:51:01.935755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Clean Memory**","metadata":{}},{"cell_type":"code","source":"def clean_memory():\n    # malloc_trim: ÌòÑÏû¨ ÏÇ¨Ïö©ÎêòÏßÄ ÏïäÎäî Î©îÎ™®Î¶¨Î•º ÏãúÏä§ÌÖúÏóêÏÑú Îã§Ïãú Î∞òÌôòÌï®0\n    ctypes.CDLL('libc.so.6').malloc_trim(0)\n    gc.collect()\nclean_memory()","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:51:01.939386Z","iopub.execute_input":"2024-03-06T07:51:01.939690Z","iopub.status.idle":"2024-03-06T07:51:02.113948Z","shell.execute_reply.started":"2024-03-06T07:51:01.939650Z","shell.execute_reply":"2024-03-06T07:51:02.112971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#A51C30\"><b><span style='color:#FFFFFF'>3 |</span></b> <b>Load Train Data</b></div>","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/hms-harmful-brain-activity-classification/train.csv\")\n#train=pd.read_csv('/Users/tahsplique/Desktop/EEG_classification/hms-harmful-brain-activity-classification/train.csv')\nprint('train shape: ',train.shape)\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:51:02.114983Z","iopub.execute_input":"2024-03-06T07:51:02.115310Z","iopub.status.idle":"2024-03-06T07:51:02.311427Z","shell.execute_reply.started":"2024-03-06T07:51:02.115284Z","shell.execute_reply":"2024-03-06T07:51:02.310450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#A51C30'> 3.1 </span> Raw EEG Signals</b>","metadata":{}},{"cell_type":"code","source":"New_Chain = True","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:51:02.312794Z","iopub.execute_input":"2024-03-06T07:51:02.313233Z","iopub.status.idle":"2024-03-06T07:51:02.318144Z","shell.execute_reply.started":"2024-03-06T07:51:02.313191Z","shell.execute_reply":"2024-03-06T07:51:02.317160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_parquet('/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/1000913311.parquet')\n#df=pd.read_parquet('/Users/tahsplique/Desktop/EEG_classification/hms-harmful-brain-activity-classification/train_eegs/1000913311.parquet')\nFEATS = df.columns\nprint(f'There are {len(FEATS)} raw eeg features')\nprint(list(FEATS))","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:51:02.319794Z","iopub.execute_input":"2024-03-06T07:51:02.320072Z","iopub.status.idle":"2024-03-06T07:51:02.441116Z","shell.execute_reply.started":"2024-03-06T07:51:02.320048Z","shell.execute_reply":"2024-03-06T07:51:02.440140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if New_Chain: \n    print('We will use the follwing subset of 10 raw eeg features:')\n    FEATS = ['Fp1','T3','C3','O1','Fp2','C4','T4','O2','Fz', 'Pz','EKG']\n    FEAT2IDX = {x:y for x,y in zip(FEATS, range(len(FEATS)))}\n    print(FEATS)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:51:02.442449Z","iopub.execute_input":"2024-03-06T07:51:02.442741Z","iopub.status.idle":"2024-03-06T07:51:02.448507Z","shell.execute_reply.started":"2024-03-06T07:51:02.442716Z","shell.execute_reply":"2024-03-06T07:51:02.447470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def eeg_from_parquet(parquet_path, display=False):\n    \n    # ÌäπÏ†ï electrodeÎßå Ï∂îÏ∂úÌïòÍ∏∞\n    eeg = pd.read_parquet(parquet_path, columns=FEATS)\n    rows = len(eeg)\n    offset = (rows-10000)//2\n    # eegÏùò Íµ¨Í∞Ñ: 50Ï¥à\n    eeg = eeg.iloc[offset:offset+10_000]\n    \n    if display:\n        plt.figure(figsize=(10,5))\n        offset = 0\n        \n    # CONVERT TO NUMPY\n    data = np.zeros((10_000, len(FEATS)))\n    for j,col in enumerate(FEATS):\n        \n        # FILL NAN\n        x = eeg[col].values.astype('float32')\n        m = np.nanmean(x)\n        if np.isnan(x).mean()<1: x = np.nan_to_num(x,nan=m)\n        else: x[:] = 0\n            \n        data[:,j] = x    \n        \n        if display: \n            if j!=0: offset += x.max()\n            plt.plot(range(10_000), x-offset, label=col)\n            offset -= x.min()\n            \n    if display:\n        plt.legend()\n        name = parquet_path.split('/')[-1]\n        name = name.split('.')[0]\n        plt.title(f'EEG {name}', size=16)\n        plt.show()\n        \n    return data    ","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:51:02.449731Z","iopub.execute_input":"2024-03-06T07:51:02.450651Z","iopub.status.idle":"2024-03-06T07:51:02.462309Z","shell.execute_reply.started":"2024-03-06T07:51:02.450619Z","shell.execute_reply":"2024-03-06T07:51:02.461363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\nCREATE_EEGS = True\nSAVE_EEGS = True\nall_eegs = {}\nDISPLAY = 4\nEEG_IDS = train.eeg_id.unique()\nPATH = '/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/'\n#PATH='/Users/tahsplique/Desktop/EEG_classification/hms-harmful-brain-activity-classification/train_eegs/'\n\nfor i, eeg_id in enumerate(EEG_IDS):\n    if (i%100==0)&(i!=0): print(i, ', ', end='')\n        \n    data = eeg_from_parquet(f'{PATH}{eeg_id}.parquet', display=i<DISPLAY) \n    all_eegs[eeg_id] = data\n    \n    if i==DISPLAY:\n        if CREATE_EEGS:\n            print(f'processing {train.eeg_id.nunique()} eeg parquets... ', end='')\n        else:\n            print(f'Reading {len(EEG_IDS)} eeg Numpys from disk.')\n            \n            break\n\n# if SAVE_EEGS:\n#     np.save('/path/to/save/all_eegs.npy', all_eegs)\n\n\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:51:02.463450Z","iopub.execute_input":"2024-03-06T07:51:02.463766Z","iopub.status.idle":"2024-03-06T07:57:34.418937Z","shell.execute_reply.started":"2024-03-06T07:51:02.463742Z","shell.execute_reply":"2024-03-06T07:57:34.418096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# all_eegs\n# np.save('/Users/tahsplique/Desktop/EEG_classification/all_eegs.npy', all_eegs)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:34.420149Z","iopub.execute_input":"2024-03-06T07:57:34.420430Z","iopub.status.idle":"2024-03-06T07:57:34.424803Z","shell.execute_reply.started":"2024-03-06T07:57:34.420406Z","shell.execute_reply":"2024-03-06T07:57:34.423788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#A51C30'> 3.2 </span> Deduplicate Train EEG Id</b>","metadata":{}},{"cell_type":"code","source":"# LOAD TRAIN \ndf = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/train.csv')\n#df=pd.read_csv('/Users/tahsplique/Desktop/EEG_classification/hms-harmful-brain-activity-classification/train.csv')\nTARGETS = df.columns[-6:]\nTARS = {'Seizure':0, 'LPD':1, 'GPD':2, 'LRDA':3, 'GRDA':4, 'Other':5}\nTARS2 = {x:y for y,x in TARS.items()}\n\ntrain = df.groupby('eeg_id')[['patient_id']].agg('first')\n\ntmp = df.groupby('eeg_id')[TARGETS].agg('sum')\nfor t in TARGETS:\n    train[t] = tmp[t].values\n    \ny_data = train[TARGETS].values\ny_data = y_data / y_data.sum(axis=1,keepdims=True)\ntrain[TARGETS] = y_data\n\ntmp = df.groupby('eeg_id')[['expert_consensus']].agg('first')\ntrain['target'] = tmp\n\ntrain = train.reset_index()\ntrain = train.loc[train.eeg_id.isin(EEG_IDS)]\nprint('Train Data with unique eeg_id shape:', train.shape )\ntrain.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:34.426371Z","iopub.execute_input":"2024-03-06T07:57:34.426657Z","iopub.status.idle":"2024-03-06T07:57:34.753137Z","shell.execute_reply.started":"2024-03-06T07:57:34.426632Z","shell.execute_reply":"2024-03-06T07:57:34.752162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Expert_Agreement**","metadata":{}},{"cell_type":"markdown","source":"### <b><span style='color:#A51C30'> 3.3 </span> Butter Low-Pass Filter[0~30Hz]</b>","metadata":{}},{"cell_type":"code","source":"from scipy.signal import butter, lfilter\n\ndef butter_lowpass_filter(data, cutoff_freq=30, sampling_rate=200, order=4):\n    nyquist = 0.5 * sampling_rate\n    normal_cutoff = cutoff_freq / nyquist\n    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n    filtered_data = lfilter(b, a, data, axis=0)\n    return filtered_data\n\ndef butter_highpass_filter(data, cutoff_freq=0.5, sampling_rate=200, order=4):\n    nyquist = 0.5 * sampling_rate\n    normal_cutoff = cutoff_freq / nyquist\n    b, a = butter(order, normal_cutoff, btype='high', analog=False)\n    filtered_data = lfilter(b, a, data, axis=0)\n    return filtered_data\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:34.758769Z","iopub.execute_input":"2024-03-06T07:57:34.759375Z","iopub.status.idle":"2024-03-06T07:57:34.814556Z","shell.execute_reply.started":"2024-03-06T07:57:34.759337Z","shell.execute_reply":"2024-03-06T07:57:34.813855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#A51C30'> 3.4 </span> Wavelet Denoising[db8]</b>","metadata":{}},{"cell_type":"code","source":"import pywt\nprint(\"The wavelet functions we can use:\")\nprint(pywt.wavelist())\n\nUSE_WAVELET = 'db8'","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:34.815744Z","iopub.execute_input":"2024-03-06T07:57:34.816443Z","iopub.status.idle":"2024-03-06T07:57:34.850610Z","shell.execute_reply.started":"2024-03-06T07:57:34.816416Z","shell.execute_reply":"2024-03-06T07:57:34.849766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# DENOISE FUNCTION\ndef maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise(x, wavelet='haar', level=1):    \n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    ret=pywt.waverec(coeff, wavelet, mode='per')\n    \n    return ret","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:34.851860Z","iopub.execute_input":"2024-03-06T07:57:34.852971Z","iopub.status.idle":"2024-03-06T07:57:34.861080Z","shell.execute_reply.started":"2024-03-06T07:57:34.852936Z","shell.execute_reply":"2024-03-06T07:57:34.860123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FREQS = [1,2,4,8,16][::-1]\nx = [all_eegs[EEG_IDS[0]][:,0]]\nfor k in FREQS:\n    x.append( butter_lowpass_filter(x[0], cutoff_freq=k) )\n\nplt.figure(figsize=(20,20))\nplt.plot(range(10_000),x[0], label='without filter')\nfor k in range(1,len(x)):\n    plt.plot(range(10_000),x[k]-k*(x[0].max()-x[0].min()), label=f'with filter {FREQS[k-1]}Hz')\nplt.legend()\nplt.title('Butter Low-Pass Filter Examples',size=18)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:34.862300Z","iopub.execute_input":"2024-03-06T07:57:34.862598Z","iopub.status.idle":"2024-03-06T07:57:35.726550Z","shell.execute_reply.started":"2024-03-06T07:57:34.862575Z","shell.execute_reply":"2024-03-06T07:57:35.725410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#A51C30\"><b><span style='color:#FFFFFF'>4 |</span></b> <b>Data Loader</b></div>","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n\nclass DataGenerator(tf.keras.utils.Sequence):\n    # Constructor(init method)\n    def __init__(self,data, batch_size=32, shuffle=True, eegs=all_eegs,\n               mode='train', downsample=5,):\n        \n        self.data = data\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.eegs = eegs \n        self.mode = mode\n        self.downsample = downsample\n        self.on_epoch_end()\n        \n    def __len__(self):\n        ct = int(np.ceil(len(self.data)/self.batch_size))\n        return ct\n        \n    def __getitem__(self, index):\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        X, y = self.__data_generation(indexes)\n        return X[:,::self.downsample,:], y\n        # return X, y\n        \n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.data))\n        if self.shuffle: np.random.shuffle(self.indexes)\n            \n    \n    def remove_ekg(self, data):\n        data[:, :-1] = data[:, :-1] - data[:, -1:]\n        data = data[:, :-1]\n\n        return data \n        \n    def __data_generation(self, indexes):\n        # New chain change 8 -> 10\n        X = np.zeros((len(indexes),10_000,11), dtype='float32')\n        y = np.zeros((len(indexes),6),dtype='float32')\n        \n        sample = np.zeros((10_000, 11))\n        for j,i in enumerate(indexes):\n            row = self.data.iloc[i]\n            data = self.eegs[row.eeg_id] \n    \n            sample=data[:,:]\n#             # FEATURE ENGINEER\n#             sample[:,0] = data[:,FEAT2IDX['Fp1']] - data[:,FEAT2IDX['T3']]\n#             sample[:,1] = data[:,FEAT2IDX['T3']] - data[:,FEAT2IDX['O1']]\n            \n#             sample[:,2] = data[:,FEAT2IDX['Fp1']] - data[:,FEAT2IDX['C3']]\n#             sample[:,3] = data[:,FEAT2IDX['C3']] - data[:,FEAT2IDX['O1']]\n            \n#             sample[:,4] = data[:,FEAT2IDX['Fp2']] - data[:,FEAT2IDX['C4']]\n#             sample[:,5] = data[:,FEAT2IDX['C4']] - data[:,FEAT2IDX['O2']]\n            \n#             sample[:,6] = data[:,FEAT2IDX['Fp2']] - data[:,FEAT2IDX['T4']]\n#             sample[:,7] = data[:,FEAT2IDX['T4']] - data[:,FEAT2IDX['O2']]\n            \n#             sample[:,8] = data[:,FEAT2IDX['Fz']] - data[:,FEAT2IDX['Pz']]\n#             sample[:,9] = data[:,FEAT2IDX['C3']] - data[:,FEAT2IDX['C4']]\n            \n            \n            # crop scaling\n            sample = np.clip(sample, -1024,1024)\n            \n            # standarization: (x-mean)/std\n            # The mean of all the raw data: 0\n            # The std of all the raw data: 32\n            sample = np.nan_to_num(sample,nan=0) / 32.0\n            \n            # BUTTER LOW-PASS FILTER[0~30Hz]\n            sample = butter_lowpass_filter(sample)\n            \n            # BUTTER HIGH-PASS FILTER[0.1~]\n            sample = butter_highpass_filter(sample)\n            \n            # Wavelet Filtering[db6]\n            sample = denoise(sample,wavelet=USE_WAVELET)\n            \n#             print (f\" this is the sample shape: {sample.shape}\")\n            \n            \n        \n            X[j,] = sample\n            \n            \n            \n            \n            if self.mode!= 'test':\n                y[j] = row[TARGETS]\n        \n#         print()\n        \n       \n        \n#         print(f\"{X.shape} this is the X shape:\")\n        \n        # last_column = X[:,:, -1:]  # Get the last column\n        # X_new= X[:,:, :-1]  # Remove the last column\n        # X_new -= last_column \n        \n#         print(f\"{X_new.shape} this is the X shape:\")\n\n        \n                \n        return X,y\n    ","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:35.727968Z","iopub.execute_input":"2024-03-06T07:57:35.728332Z","iopub.status.idle":"2024-03-06T07:57:35.750643Z","shell.execute_reply.started":"2024-03-06T07:57:35.728303Z","shell.execute_reply":"2024-03-06T07:57:35.749788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\nimport tensorflow.keras.backend as K, gc \n\ngkf = GroupKFold(n_splits=5)\nfor i, (train_idx, valid_idx) in enumerate(gkf.split(train, train.target, train.patient_id)):\n    \n    # ÏßÑÌñâÏÉÅÌÉú ÌôïÏù∏ÌïòÍ∏∞ \n    print('#'*25)\n    print(f'### Fold {i+1}')\n    print(f'### train size {len(train_idx)}, valid size {len(valid_idx)}')\n    print('#'*25)  \n          \n    # split train & valid       \n    train_gen = DataGenerator(train.iloc[train_idx], shuffle=True, batch_size=32)\n    valid_gen = DataGenerator(train.iloc[valid_idx], shuffle=False, batch_size=64)\n    \n    x, y = next(iter(train_gen))\n    print(x.shape)\n    print(y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:35.752032Z","iopub.execute_input":"2024-03-06T07:57:35.752401Z","iopub.status.idle":"2024-03-06T07:57:36.773426Z","shell.execute_reply.started":"2024-03-06T07:57:35.752368Z","shell.execute_reply":"2024-03-06T07:57:36.772436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#A51C30\"><b><span style='color:#FFFFFF'>5 |</span></b> <b>Resnet18</b></div>","metadata":{}},{"cell_type":"code","source":"# TRAIN SCHEDULE\n\ndef lrfn(epoch):\n    return [1e-3,1e-3,1e-4,1e-4,1e-5][epoch]\nLR = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\nEPOCHS = 5\n\n\n\n# class DataGenerator(Sequence):\n#     def __init__(self, x_set, y_set, batch_size):\n#         self.x, self.y = x_set, y_set\n#         self.batch_size = batch_size\n\n#     def __len__(self):\n#         return int(np.ceil(len(self.x) / float(self.batch_size)))\n\n#     def __getitem__(self, idx):\n#         batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n#         batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n#         return batch_x.astype(np.float32), batch_y #makes sure output isin float32\n    \n    \nclass CleanMemory(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    gc.collect()\n\n    \nclass ResetLR(tf.keras.callbacks.Callback):\n  def on_train_begin(self, logs={}):\n    default_lr = 0.1\n    previous_lr = self.model.optimizer.lr.read_value()\n    if previous_lr!=0.01:\n      print(\"Resetting learning rate from {} to {}\".format(previous_lr, default_lr))\n      self.model.optimizer.lr.assign(default_lr)\n\nclass ReduceLRBacktrack(ReduceLROnPlateau):\n    def __init__(self, best_path, *args, **kwargs):\n        super(ReduceLRBacktrack, self).__init__(*args, **kwargs)\n        self.best_path = best_path\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get(self.monitor)\n        if current is None:\n            logging.warning('Reduce LR on plateau conditioned on metric `%s` '\n                            'which is not available. Available metrics are: %s',\n                             self.monitor, ','.join(list(logs.keys())))\n        if not self.monitor_op(current, self.best): # not new best\n            if not self.in_cooldown(): # and we're not in cooldown\n                if self.wait+1 >= self.patience: # going to reduce lr\n                    # load best model so far\n                    print(\"Backtracking to best model before reducting LR\")\n                    self.model.load_weights(self.best_path)\n\n        super().on_epoch_end(epoch, logs) # actually reduce LR","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:36.774856Z","iopub.execute_input":"2024-03-06T07:57:36.775198Z","iopub.status.idle":"2024-03-06T07:57:36.786859Z","shell.execute_reply.started":"2024-03-06T07:57:36.775169Z","shell.execute_reply":"2024-03-06T07:57:36.785902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfrom tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense,GlobalAveragePooling1D)\nfrom tensorflow.keras.models import Model\nimport numpy as np\nimport os\n\n\n\nclass ResidualUnit(object):\n    \"\"\"Residual unit block (unidimensional).\n    Parameters\n    ----------\n    n_samples_out: int\n        Number of output samples.\n    n_filters_out: int\n        Number of output filters.\n    kernel_initializer: str, optional\n        Initializer for the weights matrices. See Keras initializers. By default it uses\n        'he_normal'.\n    dropout_keep_prob: float [0, 1), optional\n        Dropout rate used in all Dropout layers. Default is 0.8\n    kernel_size: int, optional\n        Kernel size for convolutional layers. Default is 17.\n    preactivation: bool, optional\n        When preactivation is true use full preactivation architecture proposed\n        in [1]. Otherwise, use architecture proposed in the original ResNet\n        paper [2]. By default it is true.\n    postactivation_bn: bool, optional\n        Defines if you use batch normalization before or after the activation layer (there\n        seems to be some advantages in some cases:\n        https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md).\n        If true, the batch normalization is used before the activation\n        function, otherwise the activation comes first, as it is usually done.\n        By default it is false.\n    activation_function: string, optional\n        Keras activation function to be used. By default 'relu'.\n    References\n    ----------\n    .. [1] K. He, X. Zhang, S. Ren, and J. Sun, \"Identity Mappings in Deep Residual Networks,\"\n           arXiv:1603.05027 [cs], Mar. 2016. https://arxiv.org/pdf/1603.05027.pdf.\n    .. [2] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep Residual Learning for Image Recognition,\" in 2016 IEEE Conference\n           on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778. https://arxiv.org/pdf/1512.03385.pdf\n    \"\"\"\n\n    def __init__(self, n_samples_out, n_filters_out, kernel_initializer='he_normal',\n                 dropout_keep_prob=0.8, kernel_size=17, preactivation=True,\n                 postactivation_bn=False, activation_function='relu'):\n        self.n_samples_out = n_samples_out\n        self.n_filters_out = n_filters_out\n        self.kernel_initializer = kernel_initializer\n        self.dropout_rate = 1 - dropout_keep_prob\n        self.kernel_size = kernel_size\n        self.preactivation = preactivation\n        self.postactivation_bn = postactivation_bn\n        self.activation_function = activation_function\n\n    def _skip_connection(self, y, downsample, n_filters_in):\n        \"\"\"Implement skip connection.\"\"\"\n        # Deal with downsampling\n        if downsample > 1:\n            y = MaxPooling1D(downsample, strides=downsample, padding='same')(y)\n        elif downsample == 1:\n            y = y\n        else:\n            raise ValueError(\"Number of samples should always decrease.\")\n        # Deal with n_filters dimension increase\n        if n_filters_in != self.n_filters_out:\n            # This is one of the two alternatives presented in ResNet paper\n            # Other option is to just fill the matrix with zeros.\n            y = Conv1D(self.n_filters_out, 1, padding='same',\n                       use_bias=False, kernel_initializer=self.kernel_initializer)(y)\n        return y\n\n    def _batch_norm_plus_activation(self, x):\n        if self.postactivation_bn:\n            x = Activation(self.activation_function)(x)\n            x = BatchNormalization(center=False, scale=False)(x)\n        else:\n            x = BatchNormalization()(x)\n            x = Activation(self.activation_function)(x)\n        return x\n\n    def __call__(self, inputs):\n        \"\"\"Residual unit.\"\"\"\n        x, y = inputs\n        n_samples_in = y.shape[1]\n        downsample = n_samples_in // self.n_samples_out\n        n_filters_in = y.shape[2]\n        y = self._skip_connection(y, downsample, n_filters_in)\n        # 1st layer\n        x = Conv1D(self.n_filters_out, self.kernel_size, padding='same',\n                   use_bias=False, kernel_initializer=self.kernel_initializer)(x)\n        x = self._batch_norm_plus_activation(x)\n        if self.dropout_rate > 0:\n            x = Dropout(self.dropout_rate)(x)\n\n        # 2nd layer\n        x = Conv1D(self.n_filters_out, self.kernel_size, strides=downsample,\n                   padding='same', use_bias=False,\n                   kernel_initializer=self.kernel_initializer)(x)\n        if self.preactivation:\n            x = Add()([x, y])  # Sum skip connection and main connection\n            y = x\n            x = self._batch_norm_plus_activation(x)\n            if self.dropout_rate > 0:\n                x = Dropout(self.dropout_rate)(x)\n        else:\n            x = BatchNormalization()(x)\n            x = Add()([x, y])  # Sum skip connection and main connection\n            x = Activation(self.activation_function)(x)\n            if self.dropout_rate > 0:\n                x = Dropout(self.dropout_rate)(x)\n            y = x\n        return [x, y]\n\n\ndef get_model2(n_classes,input, last_layer='softmax'): #try with softmax 'sigmoid' initially\n    kernel_size = 16 \n    kernel_initializer = 'he_normal'\n    signal = Input(shape=(input, 11), dtype=np.float32, name='signal') #change to my input size 2500,12\n    x = signal\n    x = Conv1D(64, kernel_size, padding='same', use_bias=False,\n               kernel_initializer=kernel_initializer)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x, y = ResidualUnit(1024, 128, kernel_size=kernel_size,\n                        kernel_initializer=kernel_initializer)([x, x])\n    x, y = ResidualUnit(256, 196, kernel_size=kernel_size,\n                        kernel_initializer=kernel_initializer)([x, y])\n    x, y = ResidualUnit(64, 256, kernel_size=kernel_size,\n                        kernel_initializer=kernel_initializer)([x, y])\n    x, _ = ResidualUnit(16, 320, kernel_size=kernel_size,\n                        kernel_initializer=kernel_initializer)([x, y])\n    # x=GlobalAveragePooling1D()(x)\n    x = Flatten()(x)\n    diagn = Dense(n_classes, activation=last_layer, kernel_initializer=kernel_initializer, dtype='float32')(x)\n    # diagn=GlobalAveragePooling1D()(x)\n\n    model = Model(signal, diagn)\n    return model\n\n\nmodel = get_model2(6,2000)\nmodel.summary()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:36.788029Z","iopub.execute_input":"2024-03-06T07:57:36.788375Z","iopub.status.idle":"2024-03-06T07:57:37.140641Z","shell.execute_reply.started":"2024-03-06T07:57:36.788351Z","shell.execute_reply":"2024-03-06T07:57:37.139702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#A51C30\"><b><span style='color:#FFFFFF'>6 |</span></b> <b>Cross Validation</b></div>","metadata":{}},{"cell_type":"markdown","source":"### <b><span style='color:#A51C30'> 6.1 </span> Train GroupKFold</b>","metadata":{}},{"cell_type":"code","source":"# directory_path='/kaggle/working/'\n# condition_name='resnet18v2'\ndirectory_path = 'WaveNet_Model/'\nif not os.path.exists(directory_path):\n       os.makedirs(directory_path)\n\nTRAIN_MODEL = True      \nFOLDS_TO_TRAIN = 5\nconfig= {'epochs':50, 'learning_rate':0.001}\n\nfrom sklearn.model_selection import GroupKFold\nimport tensorflow.keras.backend as K, gc \n\nall_oof = [] ; all_true = [] \n\ngkf = GroupKFold(n_splits=5)\nfor i, (train_idx, valid_idx) in enumerate(gkf.split(train, train.target, train.patient_id)):\n    \n    # ÏßÑÌñâÏÉÅÌÉú ÌôïÏù∏ÌïòÍ∏∞ \n    print('#'*25)\n    print(f'### Fold {i+1}')\n    print(f'### train size {len(train_idx)}, valid size {len(valid_idx)}')\n    print('#'*25)  \n          \n    # split train & valid       \n    train_gen = DataGenerator(train.iloc[train_idx], shuffle=True, batch_size=32)\n    valid_gen = DataGenerator(train.iloc[valid_idx], shuffle=False, batch_size=64)\n    \n    x, y = next(iter(train_gen))\n    print(x.shape)    \n    \n\n    # model Ï†ïÏùò\n    # WaveNet ModelÏùÄ Ïù¥ÎØ∏ functionÏúºÎ°ú Ï†ïÌï®\n    K.clear_session()\n    with strategy.scope():\n\n        lr = config['learning_rate']\n        opt = Adam(lr)\n        opt = mixed_precision.LossScaleOptimizer(opt)\n\n        loss = tf.keras.losses.KLDivergence()\n\n        model = get_model2(6,2000)\n        model.compile(loss=loss, optimizer=opt, metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc')], run_eagerly=False)\n\n\n          \n    # moodel fit\n    if TRAIN_MODEL:\n          \n        \n        \n        callbacks = [ReduceLROnPlateau(monitor='val_loss',\n                               factor=0.1,\n                               patience=7,\n                               min_lr=lr / 100),\n             EarlyStopping(patience=9,  # Patience should be larger than the one in ReduceLROnPlateau\n                           min_delta=0.00001)]\n        \n        model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])#remplacer par tes metriques\n\n     \n        \n        \n#         c1 = ModelCheckpoint(os.path.join(out_dir,'{}_benchmark/'.format(condition_name), 'best_model.h5'), \n#                     save_best_only=True,\n#                     save_weights_on=True,\n#                     monitor='val_loss')\n        \n        \n        \n        callbacks +=[CleanMemory()]\n        callbacks += [TensorBoard(log_dir='./logs', write_graph=False),\n                 CSVLogger('training.log', append=False)]  # Change append to true if continuing training\n        \n        model.fit(train_gen, verbose=1,\n               validation_data = valid_gen, \n              epochs=EPOCHS,\n              callbacks=callbacks)\n        # Check if the file exists and delete it if it doe\n    # model save\n    \n        file_path = f'{directory_path}WaveNet_fold{i}.weights.h5'\n        model.save_weights(file_path)\n    # save_weightsÎäî tf.kerasÏóêÏÑú ÏßÄÏõêÌïòÎäî Í∏∞Îä•\n\n                           \n    # model predict\n    oof = model.predict(valid_gen, verbose=False) \n    all_oof.append(oof)\n    all_true.append(train.iloc[valid_idx][TARGETS].values)\n                           \n    if i == FOLDS_TO_TRAIN-1: break     \n                           \nall_oof = np.concatenate(all_oof)\nall_true = np.concatenate(all_true)\n\n\n# del early_stop\n# del lrplateau\n\n# tf.keras.clear_session()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-03-06T07:57:37.142184Z","iopub.execute_input":"2024-03-06T07:57:37.142471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Data Extraction\n# ekg_HEOG = -data.iloc[:, 3].values  # HEOG artifact\n# eeg_Fz = data.iloc[:, 0].values  # Raw at Fz\n\n# # RLS ALGORITHM EXECUTION FOR Denoising\n# # Initialization\n# sample_no = len(eeg_Fz)  # No of samples/time points\n# order = 3  # Order of the Adaptive Filter (User Tunable)\n# sigma = 0.01  # Initializing variable\n# lambda_val = 0.9999  # Forgetting Factor for RLS Algorithm (User Tunable)\n# H = np.zeros(order)  # Initial filter Coefficients\n# R = sigma * np.eye(order)  # Initial value for Reference combination\n\n# correct_Fz = np.zeros_like(eeg_Fz)\n\n# # RLS Algorithm for Adaptive Denoising\n# for n in range(sample_no):\n#     s = eeg_Fz[n]  # eeg @ Fz at that time point\n#     if n >= order:\n#         r = eog_HEOG[n - order + 1:n + 1]\n\n#         # Calculation of Filter Coefficients\n#         K = np.linalg.inv(R) @ r @ np.linalg.inv(lambda_val + r @ np.linalg.inv(R) @ r)\n#         e = s - np.dot(r, H)\n#         H += K * e\n#         R = np.linalg.inv(lambda_val * np.linalg.inv(R) - lambda_val * K @ np.atleast_2d(r).T @ np.linalg.inv(R))\n\n#         # Calculation Of RLS algorithm estimated signal\n#         correct_Fz[n] = s - np.dot(r, H)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# del oof, train_gen, valid_gen\n# clean_memory()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### <b><span style='color:#A51C30'> 6.2 </span> CV Score</b>","metadata":{}},{"cell_type":"code","source":"import sys\nsys.path.append('/kaggle/input/kaggle-kl-div')\nfrom kaggle_kl_div import score\n\noof = pd.DataFrame(all_oof.copy())\noof['id'] = np.arange(len(oof))\n\ntrue = pd.DataFrame(all_true.copy())\ntrue['id'] = np.arange(len(true))\n\ncv = score(solution=true, submission=oof, row_id_column_name='id')\nprint('CV Score with WaveNet Raw EEG =',cv)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <div style=\"padding: 25px;color:white;margin:10;font-size:60%;text-align:left;display:fill;border-radius:10px;background-color:#FFFFFF;overflow:hidden;background-color:#A51C30\"><b><span style='color:#FFFFFF'>7 |</span></b> <b>Submit to Kaggle LB</b></div>","metadata":{}},{"cell_type":"code","source":"del all_eegs, train; gc.collect()\n\ntest = pd.read_csv('/kaggle/input/hms-harmful-brain-activity-classification/test.csv')\nprint('Test shape:',test.shape)\ntest.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_eegs2 = {}\nDISPLAY = 1\nEEG_IDS2 = test.eeg_id.unique()\nPATH2 = '/kaggle/input/hms-harmful-brain-activity-classification/test_eegs/'\n\nprint('Processing Test EEG parquets...'); print()\nfor i,eeg_id in enumerate(EEG_IDS2):\n        \n    # SAVE EEG TO PYTHON DICTIONARY OF NUMPY ARRAYS\n    data = eeg_from_parquet(f'{PATH2}{eeg_id}.parquet', i<DISPLAY)\n    all_eegs2[eeg_id] = data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# INFER MLP ON TEST\npreds = []\ntest_gen = DataGenerator(test, shuffle=False, batch_size=64, eegs=all_eegs2, mode='test')\n\nprint('Inferring test... ',end='')\nfor i in range(FOLDS_TO_TRAIN):\n    print(f'fold {i+1}, ',end='')\n    if TRAIN_MODEL:\n#         model.load_weights(f'WaveNet_Model/WaveNet_fold{i}.h5')\n        print(\"hello\")\n    else:\n        model = get_model2()\n        model.load_weights(f'/kaggle/input/brain-eegs/WaveNet_Model/WaveNet_fold{i}.h5')\n        #model.load_weights()\n    \n    pred = model.predict(test_gen, verbose=0)\n    preds.append(pred)\npred = np.mean(preds,axis=0)\nprint()\nprint('Test preds shape',pred.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CREATE SUBMISSION.CSV\nfrom IPython.display import display\n\nsub = pd.DataFrame({'eeg_id':test.eeg_id.values})\nsub[TARGETS] = pred\nsub.to_csv('submission.csv',index=False)\nprint('Submission shape',sub.shape)\ndisplay( sub.head() )\n\n# SANITY CHECK TO CONFIRM PREDICTIONS SUM TO ONE\nprint('Sub row 0 sums to:',sub.iloc[0,-6:].sum())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}