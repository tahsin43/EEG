{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":59093,"databundleVersionId":7469972,"sourceType":"competition"}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%pip install iterative-stratification\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:04:35.458817Z","iopub.execute_input":"2024-02-21T19:04:35.459441Z","iopub.status.idle":"2024-02-21T19:04:49.451840Z","shell.execute_reply.started":"2024-02-21T19:04:35.459382Z","shell.execute_reply":"2024-02-21T19:04:49.450525Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: iterative-stratification in /opt/conda/lib/python3.10/site-packages (0.1.7)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from iterative-stratification) (1.24.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from iterative-stratification) (1.11.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from iterative-stratification) (1.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->iterative-stratification) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->iterative-stratification) (3.2.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"# # This Python 3 environment comes with many helpful analytics libraries installed\n# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# # For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# # Input data files are available in the read-only \"../input/\" directory\n# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-02-21T19:04:49.454666Z","iopub.execute_input":"2024-02-21T19:04:49.455115Z","iopub.status.idle":"2024-02-21T19:04:49.462837Z","shell.execute_reply.started":"2024-02-21T19:04:49.455072Z","shell.execute_reply":"2024-02-21T19:04:49.461828Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import mixed_precision \nimport os\nimport gc\nimport ctypes\n\nimport numpy as np, pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport tensorflow as tf\n\nfrom tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense,GlobalAveragePooling1D)\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import (ModelCheckpoint, TensorBoard, ReduceLROnPlateau,\n                                      CSVLogger, EarlyStopping)\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.utils import to_categorical\nfrom collections import Counter\nfrom typing import Tuple, Optional, Callable\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.losses import KLDivergence\nfrom tensorflow.keras.utils import Sequence\nimport pandas as pd \n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:04:49.463880Z","iopub.execute_input":"2024-02-21T19:04:49.464196Z","iopub.status.idle":"2024-02-21T19:04:58.919693Z","shell.execute_reply.started":"2024-02-21T19:04:49.464169Z","shell.execute_reply":"2024-02-21T19:04:58.918541Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-02-21 19:04:51.138055: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-02-21 19:04:51.138211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-02-21 19:04:51.262735: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# GPU set up","metadata":{}},{"cell_type":"code","source":"\nprint('tensorflow version:',tf.__version__)\n\n# CUDA 0,1\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'\n\n# gpu strategy\ngpus = tf.config.list_physical_devices('GPU')\nif len(gpus) <= 1:\n    strategy = tf.distribute.OneDeviceStrategy(device='/gpu:0')\n    print(f'Using {len(gpus)} gpus')\nelse: \n    strategy = tf.distribute.MirroredStrategy()\n    print(f'Using {len(gpus)} gpus')\n    \n# warning filtering\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ngpu_memory_fraction = 0.8\ngpu_options = tf.compat.v1.GPUOptions(per_process_gpu_memory_fraction=gpu_memory_fraction)\nsession = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(gpu_options=gpu_options))\n\n\npolicy = mixed_precision.Policy('mixed_float16')\nmixed_precision.set_global_policy(policy)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:04:58.922249Z","iopub.execute_input":"2024-02-21T19:04:58.922858Z","iopub.status.idle":"2024-02-21T19:04:58.939486Z","shell.execute_reply.started":"2024-02-21T19:04:58.922827Z","shell.execute_reply":"2024-02-21T19:04:58.937902Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"tensorflow version: 2.15.0\nUsing 0 gpus\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Preprocessing and SET UP","metadata":{}},{"cell_type":"code","source":"train_csv = pd.read_csv(\"/kaggle/input/hms-harmful-brain-activity-classification/train.csv\")\nprint('train shape: ',train_csv.shape)\ndisplay(train_csv.head())\nTARGETS = train_csv.columns[-6:]\nTARGETS","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:04:58.941015Z","iopub.execute_input":"2024-02-21T19:04:58.941391Z","iopub.status.idle":"2024-02-21T19:04:59.213029Z","shell.execute_reply.started":"2024-02-21T19:04:58.941328Z","shell.execute_reply":"2024-02-21T19:04:59.211815Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"train shape:  (106800, 15)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       eeg_id  eeg_sub_id  eeg_label_offset_seconds  spectrogram_id  \\\n0  1628180742           0                       0.0          353733   \n1  1628180742           1                       6.0          353733   \n2  1628180742           2                       8.0          353733   \n3  1628180742           3                      18.0          353733   \n4  1628180742           4                      24.0          353733   \n\n   spectrogram_sub_id  spectrogram_label_offset_seconds    label_id  \\\n0                   0                               0.0   127492639   \n1                   1                               6.0  3887563113   \n2                   2                               8.0  1142670488   \n3                   3                              18.0  2718991173   \n4                   4                              24.0  3080632009   \n\n   patient_id expert_consensus  seizure_vote  lpd_vote  gpd_vote  lrda_vote  \\\n0       42516          Seizure             3         0         0          0   \n1       42516          Seizure             3         0         0          0   \n2       42516          Seizure             3         0         0          0   \n3       42516          Seizure             3         0         0          0   \n4       42516          Seizure             3         0         0          0   \n\n   grda_vote  other_vote  \n0          0           0  \n1          0           0  \n2          0           0  \n3          0           0  \n4          0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eeg_id</th>\n      <th>eeg_sub_id</th>\n      <th>eeg_label_offset_seconds</th>\n      <th>spectrogram_id</th>\n      <th>spectrogram_sub_id</th>\n      <th>spectrogram_label_offset_seconds</th>\n      <th>label_id</th>\n      <th>patient_id</th>\n      <th>expert_consensus</th>\n      <th>seizure_vote</th>\n      <th>lpd_vote</th>\n      <th>gpd_vote</th>\n      <th>lrda_vote</th>\n      <th>grda_vote</th>\n      <th>other_vote</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1628180742</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>353733</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>127492639</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1628180742</td>\n      <td>1</td>\n      <td>6.0</td>\n      <td>353733</td>\n      <td>1</td>\n      <td>6.0</td>\n      <td>3887563113</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1628180742</td>\n      <td>2</td>\n      <td>8.0</td>\n      <td>353733</td>\n      <td>2</td>\n      <td>8.0</td>\n      <td>1142670488</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1628180742</td>\n      <td>3</td>\n      <td>18.0</td>\n      <td>353733</td>\n      <td>3</td>\n      <td>18.0</td>\n      <td>2718991173</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1628180742</td>\n      <td>4</td>\n      <td>24.0</td>\n      <td>353733</td>\n      <td>4</td>\n      <td>24.0</td>\n      <td>3080632009</td>\n      <td>42516</td>\n      <td>Seizure</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"Index(['seizure_vote', 'lpd_vote', 'gpd_vote', 'lrda_vote', 'grda_vote',\n       'other_vote'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"train = train_csv.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n    {'spectrogram_id':'first','spectrogram_label_offset_seconds':'min'})\ntrain.columns = ['spec_id','min']\n\ntmp = train_csv.groupby('eeg_id')[['spectrogram_id','spectrogram_label_offset_seconds']].agg(\n    {'spectrogram_label_offset_seconds':'max'})\n\ntrain['max'] = tmp\n\ntmp = train_csv.groupby('eeg_id')[['patient_id']].agg('first')\ntrain['patient_id'] = tmp\n\ntmp = train_csv.groupby('eeg_id')[TARGETS].agg('sum')\nfor t in TARGETS:\n    train[t] = tmp[t].values\n    \ny_data = train[TARGETS].values\ny_data = y_data / y_data.sum(axis=1,keepdims=True)\ntrain[TARGETS] = y_data\n\ntmp = train_csv.groupby('eeg_id')[['expert_consensus']].agg('first')\ntrain['target'] = tmp\n\ntrain = train.reset_index()\nprint('Train non-overlapp eeg_id shape:', train.shape )\ntrain.head()\n\n#save the train data\n# train.to_csv('data_nonoverlap.csv',index=False)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:04:59.214535Z","iopub.execute_input":"2024-02-21T19:04:59.215159Z","iopub.status.idle":"2024-02-21T19:04:59.320319Z","shell.execute_reply.started":"2024-02-21T19:04:59.215118Z","shell.execute_reply":"2024-02-21T19:04:59.319260Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Train non-overlapp eeg_id shape: (17089, 12)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   eeg_id     spec_id     min     max  patient_id  seizure_vote  lpd_vote  \\\n0  568657   789577333     0.0    16.0       20654           0.0  0.000000   \n1  582999  1552638400     0.0    38.0       20230           0.0  0.857143   \n2  642382    14960202  1008.0  1032.0        5955           0.0  0.000000   \n3  751790   618728447   908.0   908.0       38549           0.0  0.000000   \n4  778705    52296320     0.0     0.0       40955           0.0  0.000000   \n\n   gpd_vote  lrda_vote  grda_vote  other_vote target  \n0      0.25   0.000000   0.166667    0.583333  Other  \n1      0.00   0.071429   0.000000    0.071429    LPD  \n2      0.00   0.000000   0.000000    1.000000  Other  \n3      1.00   0.000000   0.000000    0.000000    GPD  \n4      0.00   0.000000   0.000000    1.000000  Other  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>eeg_id</th>\n      <th>spec_id</th>\n      <th>min</th>\n      <th>max</th>\n      <th>patient_id</th>\n      <th>seizure_vote</th>\n      <th>lpd_vote</th>\n      <th>gpd_vote</th>\n      <th>lrda_vote</th>\n      <th>grda_vote</th>\n      <th>other_vote</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>568657</td>\n      <td>789577333</td>\n      <td>0.0</td>\n      <td>16.0</td>\n      <td>20654</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.25</td>\n      <td>0.000000</td>\n      <td>0.166667</td>\n      <td>0.583333</td>\n      <td>Other</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>582999</td>\n      <td>1552638400</td>\n      <td>0.0</td>\n      <td>38.0</td>\n      <td>20230</td>\n      <td>0.0</td>\n      <td>0.857143</td>\n      <td>0.00</td>\n      <td>0.071429</td>\n      <td>0.000000</td>\n      <td>0.071429</td>\n      <td>LPD</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>642382</td>\n      <td>14960202</td>\n      <td>1008.0</td>\n      <td>1032.0</td>\n      <td>5955</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>Other</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>751790</td>\n      <td>618728447</td>\n      <td>908.0</td>\n      <td>908.0</td>\n      <td>38549</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>1.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>GPD</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>778705</td>\n      <td>52296320</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>40955</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.00</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>Other</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\nfrom sklearn.utils import indexable, _safe_indexing\nfrom sklearn.utils.validation import _num_samples\nfrom sklearn.model_selection._split import _validate_shuffle_split\nfrom itertools import chain\nfrom sklearn.model_selection import train_test_split\n\n\n\ndef multilabel_train_test_split(*arrays,\n                                test_size=None,\n                                train_size=None,\n                                random_state=None,\n                                shuffle=True,\n                                stratify=None):\n    \"\"\"\n    Train test split for multilabel classification. Uses the algorithm from: \n    'Sechidis K., Tsoumakas G., Vlahavas I. (2011) On the Stratification of Multi-Label Data'.\n    \"\"\"\n    if stratify is None:\n        return train_test_split(*arrays, test_size=test_size,train_size=train_size,\n                                random_state=random_state, stratify=None, shuffle=shuffle)\n    \n    assert shuffle, \"Stratified train/test split is not implemented for shuffle=False\"\n    \n    n_arrays = len(arrays)\n    arrays = indexable(*arrays)\n    n_samples = _num_samples(arrays[0])\n    n_train, n_test = _validate_shuffle_split(\n        n_samples, test_size, train_size, default_test_size=0.25\n    )\n    cv = MultilabelStratifiedShuffleSplit(test_size=n_test, train_size=n_train, random_state=123)\n    train, test = next(cv.split(X=arrays[0], y=stratify))\n\n    return list(\n        chain.from_iterable(\n            (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n        )\n    )\n    \n    \n\n\ndef create_split(data_,label_col):\n    # Assuming your dataset is stored in a DataFrame called 'df'\n\n    # Split the dataset without duplicates\n    train_data_no_duplicates, temp_data_no_duplicates = multilabel_train_test_split(data_, stratify=data_[label_col], test_size=0.3, random_state=42)\n    val_data_no_duplicates, test_data_no_duplicates = multilabel_train_test_split(temp_data_no_duplicates, stratify=temp_data_no_duplicates[TARGETS] , test_size=0.66, random_state=42)\n\n    # Merge back the duplicates with the subsets\n  \n    # Merge back the special cases with the train set\n    \n\n    return train_data_no_duplicates, val_data_no_duplicates, test_data_no_duplicates","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:04:59.321566Z","iopub.execute_input":"2024-02-21T19:04:59.321849Z","iopub.status.idle":"2024-02-21T19:04:59.557629Z","shell.execute_reply.started":"2024-02-21T19:04:59.321826Z","shell.execute_reply":"2024-02-21T19:04:59.556425Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"train, val, test = create_split(train,TARGETS)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:04:59.559136Z","iopub.execute_input":"2024-02-21T19:04:59.559608Z","iopub.status.idle":"2024-02-21T19:05:00.057264Z","shell.execute_reply.started":"2024-02-21T19:04:59.559569Z","shell.execute_reply":"2024-02-21T19:05:00.055725Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import concurrent.futures\n\ndef eeg_tonumpy(df):\n    eeg_ids = df['eeg_id'].values.tolist()\n\n    directory ='/kaggle/input/hms-harmful-brain-activity-classification/train_eegs/'\n\n\n    eegs = pd.DataFrame(eeg_ids, columns=['eeg_id'])\n    eegs['file_path'] = eegs['eeg_id'].apply(lambda x: os.path.join(directory, f\"{x}.parquet\"))\n    eggs_path = eegs['file_path'].values\n\n    eeg_numpy = []\n\n    def read_eeg_data(file):\n        eeg_data = pd.read_parquet(file)\n        # Convert to numpy\n        eeg_data = eeg_data.to_numpy()\n        rows = len(eeg_data)\n        offset = (rows-10_000)//2\n        eeg = eeg_data[offset:offset+10_000]\n        \n        return eeg\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n        futures = [executor.submit(read_eeg_data, file) for file in eggs_path]\n        for future in concurrent.futures.as_completed(futures):\n            eeg_data = future.result()\n            eeg_numpy.append(eeg_data)\n                \n        \n    return  np.array(eeg_numpy)\n            \n            ","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:05:00.058742Z","iopub.execute_input":"2024-02-21T19:05:00.059161Z","iopub.status.idle":"2024-02-21T19:05:00.070197Z","shell.execute_reply.started":"2024-02-21T19:05:00.059120Z","shell.execute_reply":"2024-02-21T19:05:00.068922Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"test_eeg= eeg_tonumpy(test)\ntrain_eeg=eeg_tonumpy(train)\nval_eeg=eeg_tonumpy(val)\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:05:00.073618Z","iopub.execute_input":"2024-02-21T19:05:00.074969Z","iopub.status.idle":"2024-02-21T19:07:41.261394Z","shell.execute_reply.started":"2024-02-21T19:05:00.074926Z","shell.execute_reply":"2024-02-21T19:07:41.259019Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"#EEG with all Zeros\n\nfrom tqdm import tqdm\n\ndef get_badEEG(X_data):\n    channel_zeros_count = {f\"Channel {i+1}\": [] for i in range(X_data.shape[-1])}\n    unique_indices = set()\n\n    # Display \"analyzing\" before starting the analysis\n    print(\"Analyzing...\")\n\n    # Use tqdm for progress display\n    for sample_index, sample in tqdm(enumerate(X_data), desc=\"Samples\", dynamic_ncols=True, leave=True):\n        for i, channel in enumerate(sample.T, start=1):\n            if np.all(np.isnan(channel) | (channel == 0)):\n                channel_zeros_count[f\"Channel {i}\"].append(sample_index)\n                unique_indices.add(sample_index)\n\n    # Display \"Finished\" in highlighted green\n    print(\"\\033[92mFinished\\033[0m\")\n\n    # Print the report while iterating\n    for channel, indices in channel_zeros_count.items():\n        print(f\"{channel}: {len(indices)} samples have all zeros. Indices: {indices}\")\n\n    # Create a mask for filtering\n    mask = np.ones(len(X_data), dtype=bool)\n    mask[list(unique_indices)] = False\n    X_val_filtered = X_data[mask]\n    \n    return X_val_filtered, list(unique_indices)","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:07:41.274587Z","iopub.execute_input":"2024-02-21T19:07:41.274980Z","iopub.status.idle":"2024-02-21T19:07:41.306646Z","shell.execute_reply.started":"2024-02-21T19:07:41.274949Z","shell.execute_reply":"2024-02-21T19:07:41.304963Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_filtered,train_filtered_index = get_badEEG(train_eeg)\nval_filtered,val_filtered_index = get_badEEG(val_eeg)\ntest_filtered,test_filtered_index = get_badEEG(test_eeg)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:07:41.308226Z","iopub.execute_input":"2024-02-21T19:07:41.308634Z","iopub.status.idle":"2024-02-21T19:08:09.499069Z","shell.execute_reply.started":"2024-02-21T19:07:41.308601Z","shell.execute_reply":"2024-02-21T19:08:09.497409Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Analyzing...\n","output_type":"stream"},{"name":"stderr","text":"Samples: 11963it [00:11, 1032.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[92mFinished\u001b[0m\nChannel 1: 0 samples have all zeros. Indices: []\nChannel 2: 0 samples have all zeros. Indices: []\nChannel 3: 0 samples have all zeros. Indices: []\nChannel 4: 0 samples have all zeros. Indices: []\nChannel 5: 0 samples have all zeros. Indices: []\nChannel 6: 0 samples have all zeros. Indices: []\nChannel 7: 0 samples have all zeros. Indices: []\nChannel 8: 0 samples have all zeros. Indices: []\nChannel 9: 0 samples have all zeros. Indices: []\nChannel 10: 0 samples have all zeros. Indices: []\nChannel 11: 0 samples have all zeros. Indices: []\nChannel 12: 0 samples have all zeros. Indices: []\nChannel 13: 0 samples have all zeros. Indices: []\nChannel 14: 0 samples have all zeros. Indices: []\nChannel 15: 0 samples have all zeros. Indices: []\nChannel 16: 0 samples have all zeros. Indices: []\nChannel 17: 0 samples have all zeros. Indices: []\nChannel 18: 0 samples have all zeros. Indices: []\nChannel 19: 0 samples have all zeros. Indices: []\nChannel 20: 54 samples have all zeros. Indices: [191, 512, 586, 641, 1051, 1229, 1257, 1362, 1492, 1615, 1848, 2417, 2600, 2707, 3036, 3674, 3904, 4040, 4323, 4370, 4371, 4654, 4660, 4773, 4790, 4976, 5130, 5164, 5790, 6005, 6208, 6960, 7104, 7334, 7361, 7403, 7497, 8188, 8256, 8493, 8775, 8894, 8965, 9004, 9118, 9321, 9423, 9634, 9844, 9919, 10326, 11257, 11301, 11491]\nAnalyzing...\n","output_type":"stream"},{"name":"stderr","text":"Samples: 1746it [00:01, 1166.45it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[92mFinished\u001b[0m\nChannel 1: 0 samples have all zeros. Indices: []\nChannel 2: 0 samples have all zeros. Indices: []\nChannel 3: 0 samples have all zeros. Indices: []\nChannel 4: 0 samples have all zeros. Indices: []\nChannel 5: 0 samples have all zeros. Indices: []\nChannel 6: 0 samples have all zeros. Indices: []\nChannel 7: 0 samples have all zeros. Indices: []\nChannel 8: 0 samples have all zeros. Indices: []\nChannel 9: 0 samples have all zeros. Indices: []\nChannel 10: 0 samples have all zeros. Indices: []\nChannel 11: 0 samples have all zeros. Indices: []\nChannel 12: 0 samples have all zeros. Indices: []\nChannel 13: 0 samples have all zeros. Indices: []\nChannel 14: 0 samples have all zeros. Indices: []\nChannel 15: 0 samples have all zeros. Indices: []\nChannel 16: 0 samples have all zeros. Indices: []\nChannel 17: 0 samples have all zeros. Indices: []\nChannel 18: 0 samples have all zeros. Indices: []\nChannel 19: 0 samples have all zeros. Indices: []\nChannel 20: 9 samples have all zeros. Indices: [33, 157, 186, 327, 475, 702, 708, 813, 1197]\nAnalyzing...\n","output_type":"stream"},{"name":"stderr","text":"Samples: 3380it [00:02, 1180.91it/s]\n","output_type":"stream"},{"name":"stdout","text":"\u001b[92mFinished\u001b[0m\nChannel 1: 0 samples have all zeros. Indices: []\nChannel 2: 0 samples have all zeros. Indices: []\nChannel 3: 0 samples have all zeros. Indices: []\nChannel 4: 0 samples have all zeros. Indices: []\nChannel 5: 0 samples have all zeros. Indices: []\nChannel 6: 0 samples have all zeros. Indices: []\nChannel 7: 0 samples have all zeros. Indices: []\nChannel 8: 0 samples have all zeros. Indices: []\nChannel 9: 0 samples have all zeros. Indices: []\nChannel 10: 0 samples have all zeros. Indices: []\nChannel 11: 0 samples have all zeros. Indices: []\nChannel 12: 0 samples have all zeros. Indices: []\nChannel 13: 0 samples have all zeros. Indices: []\nChannel 14: 0 samples have all zeros. Indices: []\nChannel 15: 0 samples have all zeros. Indices: []\nChannel 16: 0 samples have all zeros. Indices: []\nChannel 17: 0 samples have all zeros. Indices: []\nChannel 18: 0 samples have all zeros. Indices: []\nChannel 19: 0 samples have all zeros. Indices: []\nChannel 20: 15 samples have all zeros. Indices: [254, 372, 670, 1261, 1563, 1926, 2106, 2180, 2232, 2451, 2764, 2849, 3155, 3302, 3345]\n","output_type":"stream"}]},{"cell_type":"code","source":"del train_eeg\ndel val_eeg\ndel test_eeg\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:10:41.589205Z","iopub.execute_input":"2024-02-21T19:10:41.589710Z","iopub.status.idle":"2024-02-21T19:10:42.306570Z","shell.execute_reply.started":"2024-02-21T19:10:41.589674Z","shell.execute_reply":"2024-02-21T19:10:42.305289Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"1688"},"metadata":{}}]},{"cell_type":"code","source":"#Filter to remove all bad EEG values\ntrain_data_filtered = train.drop(train.index[train_filtered_index])\nval_data_filtered = val.drop(val.index[val_filtered_index])\ntest_data_filtered = test.drop(test.index[test_filtered_index])\nY_train = train_data_filtered.iloc[:, 5:11]\nY_val=val_data_filtered.iloc[:, 5:11]\nY_test=test_data_filtered.iloc[:, 5:11]","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:13:48.239040Z","iopub.execute_input":"2024-02-21T19:13:48.245836Z","iopub.status.idle":"2024-02-21T19:13:48.300808Z","shell.execute_reply.started":"2024-02-21T19:13:48.245221Z","shell.execute_reply":"2024-02-21T19:13:48.295565Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#Normalizing values and removing NAN \nfrom sklearn.preprocessing import StandardScaler  #scaler   \ndef getParam(X):\n    #to float32\n    a, b, c = X.shape\n    \n    ax = np.reshape(X, (a * b * c))\n    ax = np.reshape(X, (-1, 1))\n    print(ax.shape)\n    # scaler = StandardScaler()\n    mean=np.nanmean(ax)\n    std=np.nanstd(ax)\n    \n    # scaler.fit(ax)\n    # mean = scaler.mean_\n    # std = np.sqrt(scaler.var_)\n    return mean,std\n\n\ndef standardize(X, mean, std):\n    return (X - mean) / std\n\n\nmean,std=getParam(train_filtered)\nprint(mean,std)\n\ntrain_normalized = standardize(train_filtered, mean, std)\ndel train_filtered\nval_normalized = standardize(val_filtered, mean, std)\ndel val_filtered\ntest_normalized = standardize(test_filtered, mean, std)\ndel test_filtered\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:15:25.238787Z","iopub.execute_input":"2024-02-21T19:15:25.239247Z","iopub.status.idle":"2024-02-21T19:16:09.787172Z","shell.execute_reply.started":"2024-02-21T19:15:25.239214Z","shell.execute_reply":"2024-02-21T19:16:09.785745Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"(2381800000, 1)\n79.48599 5467.9014\n","output_type":"stream"}]},{"cell_type":"code","source":"train_normalized[np.isnan(train_normalized)] =mean\nval_normalized[np.isnan(val_normalized)] = mean\ntest_normalized[np.isnan(test_normalized)] = mean","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:18:28.584236Z","iopub.execute_input":"2024-02-21T19:18:28.584711Z","iopub.status.idle":"2024-02-21T19:18:32.193724Z","shell.execute_reply.started":"2024-02-21T19:18:28.584674Z","shell.execute_reply":"2024-02-21T19:18:32.192319Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"X_train=train_normalized\nX_val=val_normalized\nX_test=test_normalized\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:34:56.958915Z","iopub.execute_input":"2024-02-21T19:34:56.959387Z","iopub.status.idle":"2024-02-21T19:34:56.965813Z","shell.execute_reply.started":"2024-02-21T19:34:56.959353Z","shell.execute_reply":"2024-02-21T19:34:56.963992Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"#  Resnet18 Model","metadata":{}},{"cell_type":"code","source":"\nfrom tensorflow.keras.layers import (Input, Conv1D, MaxPooling1D, Dropout, BatchNormalization, Activation, Add, Flatten, Dense,GlobalAveragePooling1D)\nfrom tensorflow.keras.models import Model\nimport numpy as np\nimport os\n\n\n\nclass ResidualUnit(object):\n    \"\"\"Residual unit block (unidimensional).\n    Parameters\n    ----------\n    n_samples_out: int\n        Number of output samples.\n    n_filters_out: int\n        Number of output filters.\n    kernel_initializer: str, optional\n        Initializer for the weights matrices. See Keras initializers. By default it uses\n        'he_normal'.\n    dropout_keep_prob: float [0, 1), optional\n        Dropout rate used in all Dropout layers. Default is 0.8\n    kernel_size: int, optional\n        Kernel size for convolutional layers. Default is 17.\n    preactivation: bool, optional\n        When preactivation is true use full preactivation architecture proposed\n        in [1]. Otherwise, use architecture proposed in the original ResNet\n        paper [2]. By default it is true.\n    postactivation_bn: bool, optional\n        Defines if you use batch normalization before or after the activation layer (there\n        seems to be some advantages in some cases:\n        https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md).\n        If true, the batch normalization is used before the activation\n        function, otherwise the activation comes first, as it is usually done.\n        By default it is false.\n    activation_function: string, optional\n        Keras activation function to be used. By default 'relu'.\n    References\n    ----------\n    .. [1] K. He, X. Zhang, S. Ren, and J. Sun, \"Identity Mappings in Deep Residual Networks,\"\n           arXiv:1603.05027 [cs], Mar. 2016. https://arxiv.org/pdf/1603.05027.pdf.\n    .. [2] K. He, X. Zhang, S. Ren, and J. Sun, \"Deep Residual Learning for Image Recognition,\" in 2016 IEEE Conference\n           on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 770-778. https://arxiv.org/pdf/1512.03385.pdf\n    \"\"\"\n\n    def __init__(self, n_samples_out, n_filters_out, kernel_initializer='he_normal',\n                 dropout_keep_prob=0.8, kernel_size=17, preactivation=True,\n                 postactivation_bn=False, activation_function='relu'):\n        self.n_samples_out = n_samples_out\n        self.n_filters_out = n_filters_out\n        self.kernel_initializer = kernel_initializer\n        self.dropout_rate = 1 - dropout_keep_prob\n        self.kernel_size = kernel_size\n        self.preactivation = preactivation\n        self.postactivation_bn = postactivation_bn\n        self.activation_function = activation_function\n\n    def _skip_connection(self, y, downsample, n_filters_in):\n        \"\"\"Implement skip connection.\"\"\"\n        # Deal with downsampling\n        if downsample > 1:\n            y = MaxPooling1D(downsample, strides=downsample, padding='same')(y)\n        elif downsample == 1:\n            y = y\n        else:\n            raise ValueError(\"Number of samples should always decrease.\")\n        # Deal with n_filters dimension increase\n        if n_filters_in != self.n_filters_out:\n            # This is one of the two alternatives presented in ResNet paper\n            # Other option is to just fill the matrix with zeros.\n            y = Conv1D(self.n_filters_out, 1, padding='same',\n                       use_bias=False, kernel_initializer=self.kernel_initializer)(y)\n        return y\n\n    def _batch_norm_plus_activation(self, x):\n        if self.postactivation_bn:\n            x = Activation(self.activation_function)(x)\n            x = BatchNormalization(center=False, scale=False)(x)\n        else:\n            x = BatchNormalization()(x)\n            x = Activation(self.activation_function)(x)\n        return x\n\n    def __call__(self, inputs):\n        \"\"\"Residual unit.\"\"\"\n        x, y = inputs\n        n_samples_in = y.shape[1]\n        downsample = n_samples_in // self.n_samples_out\n        n_filters_in = y.shape[2]\n        y = self._skip_connection(y, downsample, n_filters_in)\n        # 1st layer\n        x = Conv1D(self.n_filters_out, self.kernel_size, padding='same',\n                   use_bias=False, kernel_initializer=self.kernel_initializer)(x)\n        x = self._batch_norm_plus_activation(x)\n        if self.dropout_rate > 0:\n            x = Dropout(self.dropout_rate)(x)\n\n        # 2nd layer\n        x = Conv1D(self.n_filters_out, self.kernel_size, strides=downsample,\n                   padding='same', use_bias=False,\n                   kernel_initializer=self.kernel_initializer)(x)\n        if self.preactivation:\n            x = Add()([x, y])  # Sum skip connection and main connection\n            y = x\n            x = self._batch_norm_plus_activation(x)\n            if self.dropout_rate > 0:\n                x = Dropout(self.dropout_rate)(x)\n        else:\n            x = BatchNormalization()(x)\n            x = Add()([x, y])  # Sum skip connection and main connection\n            x = Activation(self.activation_function)(x)\n            if self.dropout_rate > 0:\n                x = Dropout(self.dropout_rate)(x)\n            y = x\n        return [x, y]\n\n\ndef get_model2(n_classes, last_layer='softmax'): #try with softmax 'sigmoid' initially\n    kernel_size = 16 \n    kernel_initializer = 'he_normal'\n    signal = Input(shape=(10000, 20), dtype=np.float32, name='signal') #change to my input size 2500,12\n #change to my input size 2500,12\n    x = signal\n    x = Conv1D(64, kernel_size, padding='same', use_bias=False,\n               kernel_initializer=kernel_initializer)(x)\n    x = BatchNormalization()(x)\n    x = Activation('relu')(x)\n    x, y = ResidualUnit(1024, 128, kernel_size=kernel_size,\n                        kernel_initializer=kernel_initializer)([x, x])\n    x, y = ResidualUnit(256, 196, kernel_size=kernel_size,\n                        kernel_initializer=kernel_initializer)([x, y])\n    x, y = ResidualUnit(64, 256, kernel_size=kernel_size,\n                        kernel_initializer=kernel_initializer)([x, y])\n    x, _ = ResidualUnit(16, 320, kernel_size=kernel_size,\n                        kernel_initializer=kernel_initializer)([x, y])\n    # x=GlobalAveragePooling1D()(x)\n    x = Flatten()(x)\n    diagn = Dense(n_classes, activation=last_layer, kernel_initializer=kernel_initializer, dtype='float32')(x)\n    # diagn=GlobalAveragePooling1D()(x)\n\n    model = Model(signal, diagn)\n    return model\n\n\nmodel = get_model2(6)\nmodel.summary()\n","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"execution":{"iopub.status.busy":"2024-02-21T19:32:38.443157Z","iopub.execute_input":"2024-02-21T19:32:38.444328Z","iopub.status.idle":"2024-02-21T19:32:39.363260Z","shell.execute_reply.started":"2024-02-21T19:32:38.444276Z","shell.execute_reply":"2024-02-21T19:32:39.361854Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\n Layer (type)                Output Shape                 Param #   Connected to                  \n==================================================================================================\n signal (InputLayer)         [(None, 10000, 20)]          0         []                            \n                                                                                                  \n conv1d_13 (Conv1D)          (None, 10000, 64)            20480     ['signal[0][0]']              \n                                                                                                  \n batch_normalization_9 (Bat  (None, 10000, 64)            256       ['conv1d_13[0][0]']           \n chNormalization)                                                                                 \n                                                                                                  \n activation_9 (Activation)   (None, 10000, 64)            0         ['batch_normalization_9[0][0]'\n                                                                    ]                             \n                                                                                                  \n conv1d_15 (Conv1D)          (None, 10000, 128)           131072    ['activation_9[0][0]']        \n                                                                                                  \n batch_normalization_10 (Ba  (None, 10000, 128)           512       ['conv1d_15[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_10 (Activation)  (None, 10000, 128)           0         ['batch_normalization_10[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_8 (Dropout)         (None, 10000, 128)           0         ['activation_10[0][0]']       \n                                                                                                  \n max_pooling1d_4 (MaxPoolin  (None, 1112, 64)             0         ['activation_9[0][0]']        \n g1D)                                                                                             \n                                                                                                  \n conv1d_16 (Conv1D)          (None, 1112, 128)            262144    ['dropout_8[0][0]']           \n                                                                                                  \n conv1d_14 (Conv1D)          (None, 1112, 128)            8192      ['max_pooling1d_4[0][0]']     \n                                                                                                  \n add_4 (Add)                 (None, 1112, 128)            0         ['conv1d_16[0][0]',           \n                                                                     'conv1d_14[0][0]']           \n                                                                                                  \n batch_normalization_11 (Ba  (None, 1112, 128)            512       ['add_4[0][0]']               \n tchNormalization)                                                                                \n                                                                                                  \n activation_11 (Activation)  (None, 1112, 128)            0         ['batch_normalization_11[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_9 (Dropout)         (None, 1112, 128)            0         ['activation_11[0][0]']       \n                                                                                                  \n conv1d_18 (Conv1D)          (None, 1112, 196)            401408    ['dropout_9[0][0]']           \n                                                                                                  \n batch_normalization_12 (Ba  (None, 1112, 196)            784       ['conv1d_18[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_12 (Activation)  (None, 1112, 196)            0         ['batch_normalization_12[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_10 (Dropout)        (None, 1112, 196)            0         ['activation_12[0][0]']       \n                                                                                                  \n max_pooling1d_5 (MaxPoolin  (None, 278, 128)             0         ['add_4[0][0]']               \n g1D)                                                                                             \n                                                                                                  \n conv1d_19 (Conv1D)          (None, 278, 196)             614656    ['dropout_10[0][0]']          \n                                                                                                  \n conv1d_17 (Conv1D)          (None, 278, 196)             25088     ['max_pooling1d_5[0][0]']     \n                                                                                                  \n add_5 (Add)                 (None, 278, 196)             0         ['conv1d_19[0][0]',           \n                                                                     'conv1d_17[0][0]']           \n                                                                                                  \n batch_normalization_13 (Ba  (None, 278, 196)             784       ['add_5[0][0]']               \n tchNormalization)                                                                                \n                                                                                                  \n activation_13 (Activation)  (None, 278, 196)             0         ['batch_normalization_13[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_11 (Dropout)        (None, 278, 196)             0         ['activation_13[0][0]']       \n                                                                                                  \n conv1d_21 (Conv1D)          (None, 278, 256)             802816    ['dropout_11[0][0]']          \n                                                                                                  \n batch_normalization_14 (Ba  (None, 278, 256)             1024      ['conv1d_21[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_14 (Activation)  (None, 278, 256)             0         ['batch_normalization_14[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_12 (Dropout)        (None, 278, 256)             0         ['activation_14[0][0]']       \n                                                                                                  \n max_pooling1d_6 (MaxPoolin  (None, 70, 196)              0         ['add_5[0][0]']               \n g1D)                                                                                             \n                                                                                                  \n conv1d_22 (Conv1D)          (None, 70, 256)              1048576   ['dropout_12[0][0]']          \n                                                                                                  \n conv1d_20 (Conv1D)          (None, 70, 256)              50176     ['max_pooling1d_6[0][0]']     \n                                                                                                  \n add_6 (Add)                 (None, 70, 256)              0         ['conv1d_22[0][0]',           \n                                                                     'conv1d_20[0][0]']           \n                                                                                                  \n batch_normalization_15 (Ba  (None, 70, 256)              1024      ['add_6[0][0]']               \n tchNormalization)                                                                                \n                                                                                                  \n activation_15 (Activation)  (None, 70, 256)              0         ['batch_normalization_15[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_13 (Dropout)        (None, 70, 256)              0         ['activation_15[0][0]']       \n                                                                                                  \n conv1d_24 (Conv1D)          (None, 70, 320)              1310720   ['dropout_13[0][0]']          \n                                                                                                  \n batch_normalization_16 (Ba  (None, 70, 320)              1280      ['conv1d_24[0][0]']           \n tchNormalization)                                                                                \n                                                                                                  \n activation_16 (Activation)  (None, 70, 320)              0         ['batch_normalization_16[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_14 (Dropout)        (None, 70, 320)              0         ['activation_16[0][0]']       \n                                                                                                  \n max_pooling1d_7 (MaxPoolin  (None, 18, 256)              0         ['add_6[0][0]']               \n g1D)                                                                                             \n                                                                                                  \n conv1d_25 (Conv1D)          (None, 18, 320)              1638400   ['dropout_14[0][0]']          \n                                                                                                  \n conv1d_23 (Conv1D)          (None, 18, 320)              81920     ['max_pooling1d_7[0][0]']     \n                                                                                                  \n add_7 (Add)                 (None, 18, 320)              0         ['conv1d_25[0][0]',           \n                                                                     'conv1d_23[0][0]']           \n                                                                                                  \n batch_normalization_17 (Ba  (None, 18, 320)              1280      ['add_7[0][0]']               \n tchNormalization)                                                                                \n                                                                                                  \n activation_17 (Activation)  (None, 18, 320)              0         ['batch_normalization_17[0][0]\n                                                                    ']                            \n                                                                                                  \n dropout_15 (Dropout)        (None, 18, 320)              0         ['activation_17[0][0]']       \n                                                                                                  \n flatten_1 (Flatten)         (None, 5760)                 0         ['dropout_15[0][0]']          \n                                                                                                  \n dense_1 (Dense)             (None, 6)                    34566     ['flatten_1[0][0]']           \n                                                                                                  \n==================================================================================================\nTotal params: 6437670 (24.56 MB)\nTrainable params: 6433942 (24.54 MB)\nNon-trainable params: 3728 (14.56 KB)\n__________________________________________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Training ","metadata":{}},{"cell_type":"markdown","source":"# Helpers","metadata":{}},{"cell_type":"code","source":"class DataGenerator(Sequence):\n    def __init__(self, x_set, y_set, batch_size):\n        self.x, self.y = x_set, y_set\n        self.batch_size = batch_size\n\n    def __len__(self):\n        return int(np.ceil(len(self.x) / float(self.batch_size)))\n\n    def __getitem__(self, idx):\n        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n        return batch_x.astype(np.float32), batch_y #makes sure output isin float32\n    \n    \nclass CleanMemory(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    gc.collect()\n\n    \nclass ResetLR(tf.keras.callbacks.Callback):\n  def on_train_begin(self, logs={}):\n    default_lr = 0.1\n    previous_lr = self.model.optimizer.lr.read_value()\n    if previous_lr!=0.01:\n      print(\"Resetting learning rate from {} to {}\".format(previous_lr, default_lr))\n      self.model.optimizer.lr.assign(default_lr)\n\nclass ReduceLRBacktrack(ReduceLROnPlateau):\n    def __init__(self, best_path, *args, **kwargs):\n        super(ReduceLRBacktrack, self).__init__(*args, **kwargs)\n        self.best_path = best_path\n\n    def on_epoch_end(self, epoch, logs=None):\n        current = logs.get(self.monitor)\n        if current is None:\n            logging.warning('Reduce LR on plateau conditioned on metric `%s` '\n                            'which is not available. Available metrics are: %s',\n                             self.monitor, ','.join(list(logs.keys())))\n        if not self.monitor_op(current, self.best): # not new best\n            if not self.in_cooldown(): # and we're not in cooldown\n                if self.wait+1 >= self.patience: # going to reduce lr\n                    # load best model so far\n                    print(\"Backtracking to best model before reducting LR\")\n                    self.model.load_weights(self.best_path)\n\n        super().on_epoch_end(epoch, logs) # actually reduce LR","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:35:04.416386Z","iopub.execute_input":"2024-02-21T19:35:04.416830Z","iopub.status.idle":"2024-02-21T19:35:04.431946Z","shell.execute_reply.started":"2024-02-21T19:35:04.416799Z","shell.execute_reply":"2024-02-21T19:35:04.430448Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"config= {'epochs':50, 'learning_rate':0.001, 'batch_size': 256}\nout_dir='/kaggle/working'\nloss = tf.keras.losses.KLDivergence()\ncondition_name='V1'\n\nlr = config['learning_rate']\nbatch_size =config['batch_size']\nopt = Adam(lr)\nopt = mixed_precision.LossScaleOptimizer(opt)\n\ncallbacks = [ReduceLROnPlateau(monitor='val_loss',\n                               factor=0.1,\n                               patience=7,\n                               min_lr=lr / 100),\n             EarlyStopping(patience=9,  # Patience should be larger than the one in ReduceLROnPlateau\n                           min_delta=0.00001)]\n\nmodel = get_model2(6)\nmodel.compile(loss=loss, optimizer=opt, metrics=['accuracy', #remplacer par tes metriques\n                                                                tf.keras.metrics.CategoricalAccuracy(name='acc'),\n                                                                #  tf.keras.metrics.AUC(curve='ROC', name='ROC'),\n                                                                #  tf.keras.metrics.AUC(curve='PR', name='PR')\n                                                                ], run_eagerly=False)\n\n    \n\nc1 = ModelCheckpoint(os.path.join(out_dir,'{}_benchmark/'.format(condition_name), 'best_model.h5'), \n                    save_best_only=False,\n                    save_weights_on=True,\n                    monitor='val_loss')\n    \n    \n\ntrain_gen = DataGenerator(X_train, Y_train, config['batch_size']) #if don't like go to 512\nval_gen = DataGenerator(X_val, Y_val,config['batch_size'])\n\ncallbacks +=[CleanMemory(),c1]\ncallbacks += [TensorBoard(log_dir='./logs', write_graph=False),\n                 CSVLogger('training.log', append=False)]  # Change append to true if continuing training\n\n\n    \nhistory = model.fit(train_gen,\n                    epochs=config['epochs'],\n                    initial_epoch=0,  # If you are continuing a interrupted section change here\n                    callbacks=callbacks,\n                    validation_data=val_gen,\n                    verbose=1)\n# Save final result\ndel X_train\ndel Y_train\ndel X_val\ndel Y_val\n\n\n\n# model.save_weights(os.path.join(out_dir,'{}_benchmark/'.format(condition_name), 'model.h5')) #useless if save at checkpoint\n\n\ndel model\n# del early_stop\n# del lrplateau\n# tf.keras.clear_session()\ntf.compat.v1.reset_default_graph()\ngc.collect()\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-02-21T19:35:10.532668Z","iopub.execute_input":"2024-02-21T19:35:10.533142Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}